{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DI-zD3xSjFSkfPzPAuofQbiAJ8TXXAwd",
      "authorship_tag": "ABX9TyPcO2j0cWOC4/tF39mj2Ev7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amenehmn/Text_Mining/blob/main/text_classification_by_ParsBert_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "WYYQ8aaMbuo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "# Load ParsBERT tokenizer and model with four labels\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-sentiment-deepsentipers-binary\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"HooshvareLab/bert-fa-base-uncased-sentiment-deepsentipers-binary\", num_labels=8, ignore_mismatched_sizes=True).to(device)\n",
        "\n",
        "# Load data\n",
        "Textlist = open(r\"Topic-Classification.txt\",mode='r',encoding='utf-8').read().splitlines()\n",
        "SEED = 402\n",
        "random.seed(SEED)\n",
        "Textlist = random.sample(Textlist,600)\n",
        "\n",
        "LabeledText = []\n",
        "texts = []\n",
        "labels = []\n",
        "for i in Textlist:\n",
        "    text = i.split(\"~\")[0]\n",
        "    label = i.split(\"~\")[1]\n",
        "    texts.append(text)\n",
        "    labels.append(label)\n",
        "\n",
        "# create a dictionary that maps each unique string label to an integer value\n",
        "label_dict = {}\n",
        "counter = 0\n",
        "for label in labels:\n",
        "  if label not in label_dict:\n",
        "    label_dict[label] = counter\n",
        "    counter += 1\n",
        "# create a list of int labels using the dictionary\n",
        "int_labels = []\n",
        "for label in labels:\n",
        "  int_labels.append(label_dict[label])\n",
        "\n",
        "train_texts, test_texts, train_labels , test_labels = train_test_split(texts, int_labels, test_size=0.2) # split the data into train and test sets\n",
        "\n",
        "\n",
        "# Encode data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert to tensors\n",
        "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
        "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
        "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 4\n",
        "\n",
        "# Create data loaders\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_dataloader:\n",
        "        # Get inputs, masks and labels from batch\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Backward pass and update parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "for batch in test_dataloader:\n",
        "    # Get inputs, masks and labels from batch\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    # Get logits and labels\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Append to lists\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "# Flatten lists\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Get predicted labels\n",
        "pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate accuracy score\n",
        "acc_score = accuracy_score(true_labels, pred_labels)\n",
        "print(f\"Accuracy score: {acc_score}\")\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(true_labels, pred_labels))\n",
        "\n",
        "#A test example\n",
        "PreText = [\"سوپرماریو رکوردهای فروش فیلم انیمیشن را شکست، اما دل منتقدان را راضی نکرد\"]\n",
        "\n",
        "# Encode the text and label\n",
        "preText_inputs = tokenizer(PreText, return_tensors=\"pt\").to(device)\n",
        "outputs = model(**preText_inputs)\n",
        "\n",
        "# Get the predicted label\n",
        "predicted_label = outputs.logits.argmax(-1).item()\n",
        "print(f\"The predicted label for the text is: {predicted_label}\")\n",
        "\n",
        "for key, value in label_dict.items():\n",
        "  if value == predicted_label:\n",
        "    print(\"The predicted label for the text is: %s\" % key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTxn1o6wlVqr",
        "outputId": "af567f2c-31ef-4dcd-ebae-78b4081db4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased-sentiment-deepsentipers-binary and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 0.9583333333333334\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97        20\n",
            "           1       1.00      0.80      0.89        15\n",
            "           2       0.89      1.00      0.94        16\n",
            "           3       0.95      0.95      0.95        19\n",
            "           4       1.00      1.00      1.00        15\n",
            "           5       0.91      1.00      0.95        10\n",
            "           6       0.92      1.00      0.96        12\n",
            "           7       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           0.96       120\n",
            "   macro avg       0.96      0.96      0.96       120\n",
            "weighted avg       0.96      0.96      0.96       120\n",
            "\n"
          ]
        }
      ]
    }
  ]
}