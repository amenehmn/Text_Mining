{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1oN0zR_ivDsPnmNAIBEHcfZEGeUk9n1Cm",
      "authorship_tag": "ABX9TyN/KQUmPs5vqCLaxptZmC9O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amenehmn/Text_Mining/blob/main/Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==3.8.3"
      ],
      "metadata": {
        "id": "5le4cXETM2D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "2h91pCuhRC5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7ncNfHxKCjn",
        "outputId": "dbdfd3d3-1645-4ebc-9dde-57c2e7019068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (142) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary One(extractive):\n",
            "با این حال، در این متن سعی شده است با زبانی ساده، مقدمات متن‌کاوی، تکنیک‌ها و روش‌های مختلف آن و دلیل اهمیت متن کاوی شرح داده شود.\n",
            "پس از آموزش، مدل‌های یادگیری ماشین قادر خواهند بود تا به طور خودکار و با درصد دقت مشخصی، در مورد داده‌های ورودی پیش‌بینی انجام دهند.\n",
            "همچنین، داده‌های آموزشی حتما باید منعکس کننده دامنه مسأله‌ای باشند که مدل یادگیری برای حل آن ارائه شده است.\n",
            "پس از آشنایی با مفهوم متن کاوی، در مرحله بعد، تفاوت میان مفاهیم متن‌کاوی، «تحلیل کیفی متن و «تحلیل کمی متن»  مورد بررسی قرار می‌گیرد.\n",
            "متن‌کاوی، مفاهیم آمار، زبان‌شناسی و یادگیری ماشین را ترکیب می‌کند تا مدل‌های هوشمندی برای یادگیری رفتار و مدل داده‌های آموزشی تولید کند.\n",
            "در نقطه مقابل، تحلیل کمی متن از نتایج حاصل از تحلیل‌های انجام شده توسط مدل‌های متن کاوی، برای تولید داده‌نما و انواع مختلفی از واسط‌های بصری داده استفاده می‌کند.\n",
            "در غالب موارد، مدل‌های متن کاوی با روش‌های تحلیل کمی متن ترکیب و داده‌های حاوی محتوای متنی تحلیل می‌شوند.\n",
            "\n",
            "\n",
            "Summary Two(abstractive):\n",
            "متن کاوی ( به انگلیسی : کاویing ) یک روش تجزیه و تحلیل متن است که در ان ، یک فرایند تولید ، پردازش ، پردازش و پردازش اطلاعات انجام میشود. در این روش ، فرایند تولید یک محصول ، یک محصول یا یک فرایند دیگر ، تجزیه و پردازش میشود. این فرایند معمولا به عنوان یک فرایند تولیدی یا فرایند تولید نرمافزار شناخته میشود. برای مثال ، یک شرکت نرمافزاری ، یک متن کاوی را با استفاده از یک الگوریتم ، تجزیه میکند تا یک محصول را در قالب یک الگوریتم یا برنامه ، تجزیه کند.\n"
          ]
        }
      ],
      "source": [
        "from gensim.summarization.summarizer import summarize\n",
        "from transformers import BertTokenizerFast, EncoderDecoderConfig, EncoderDecoderModel\n",
        " \n",
        "\n",
        "def extractive_method_summarization(text):\n",
        "  # Summarize the text with a ratio of 0.2 (20% of the original text)\n",
        "  summary = summarize(text, ratio=0.2)\n",
        "\n",
        "  return(summary)\n",
        "\n",
        "# Defining a function to summarize a Persian text using the model\n",
        "def abstractive_method_summarization(text):\n",
        "\n",
        "\n",
        "  model_name = 'm3hrdadfi/bert2bert-fa-wiki-summary'\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "  config = EncoderDecoderConfig.from_pretrained(model_name)\n",
        "  model = EncoderDecoderModel.from_pretrained(model_name, config=config)\n",
        "\n",
        "  sequence = text\n",
        "  inputs = tokenizer([sequence], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "  input_ids = inputs.input_ids\n",
        "  attention_mask = inputs.attention_mask\n",
        "\n",
        "  outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
        "  generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "  return generated\n",
        "\n",
        "#load_data\n",
        "Text = open(\"/content/.../Text Summarization.txt\",mode='r',encoding='utf-8').read()\n",
        "extractive_summary = extractive_method_summarization(Text)\n",
        "abstractive_summary = abstractive_method_summarization(Text)\n",
        "\n",
        "print(\"Summary One(extractive):\" )\n",
        "print(extractive_summary)\n",
        "print('\\n')\n",
        "print(\"Summary Two(abstractive):\" )\n",
        "print(abstractive_summary[0])"
      ]
    }
  ]
}